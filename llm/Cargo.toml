[package]
name = "vllora_llm"
version = "0.1.17"
authors = ["Vllora Team<api@vllora.dev>"]
description = "LLM client layer for the Vllora AI Gateway: unified chat-completions over multiple providers (OpenAI, Anthropic, Gemini, Bedrock, LangDB proxy) with optional tracing/telemetry."
license = "Apache-2.0"
repository = "https://github.com/vllora/vllora"
documentation = "https://vllora.dev/docs"
readme = "./README.md"
keywords = ["llm", "openai", "anthropic", "gemini", "mcp"]
categories = ["web-programming", "network-programming"]
edition = "2021"

[lib]
name = "vllora_llm"
path = "src/lib.rs"


[dependencies]
vllora_telemetry = { path = "../telemetry", version = "0.1.17" }

async-openai = { package = "async-openai-compat", version = "0.30.2" }
clust = { version = "0.9.10", package = "langdb_clust" }

aws-sdk-bedrock = "1.118.0"
aws-sdk-bedrockruntime = "1.111.0"
aws-smithy-types = { version = "1.3.4", features = [
  "serde-deserialize",
  "serde-serialize",
] }
aws-credential-types = "1.2.10"
aws-smithy-runtime-api = "1.9.2"
aws-config = { version = "1.8.8", features = ["behavior-version-latest"] }

serde_with = { version = "3.15.1", features = ["json"] }
serde_tuple = "1.1.2"

async-trait = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
tokio = { workspace = true }
thiserror = { workspace = true }
reqwest = { workspace = true }

rmcp = { workspace = true }

tracing = { workspace = true }
tracing-subscriber = { workspace = true }
tracing-opentelemetry = { workspace = true }
tracing-futures = { workspace = true }
opentelemetry = { workspace = true }
valuable = { workspace = true }

chrono = { workspace = true }
base64 = { workspace = true }
regex = { workspace = true }

validator = { version = "0.20", features = ["derive"] }
minijinja = "2.12.0"
futures = { workspace = true }
reqwest-eventsource = { workspace = true }
tokio-stream = { workspace = true }
uuid = { workspace = true }
rand = "0.9.2"

[features]
# Default feature set. Telemetry is exposed via `tracing` and
# `vllora_telemetry`; see `llm/examples/tracing` for a console
# tracing setup.
default = []
